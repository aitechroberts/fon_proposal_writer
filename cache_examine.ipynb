{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8deafe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found cache directory: /home/jroberts/.dspy_cache\n",
      "\n",
      "üìÅ Cache contains 48 files\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Find where DSPy stores its cache\n",
    "import dspy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Check for cache directory\n",
    "possible_paths = [\n",
    "    './cachedir',\n",
    "    './.dspy_cache', \n",
    "    '~/.dspy_cache',\n",
    "    './dspy_cache',\n",
    "    './__pycache__/dspy_cache'\n",
    "]\n",
    "\n",
    "cache_dir = None\n",
    "for path in possible_paths:\n",
    "    p = Path(path).expanduser()\n",
    "    if p.exists():\n",
    "        print(f\"‚úÖ Found cache directory: {p.absolute()}\")\n",
    "        cache_dir = p\n",
    "        break\n",
    "\n",
    "if not cache_dir:\n",
    "    # Try to get it from the disk cache object\n",
    "    if hasattr(dspy.cache.disk_cache, 'directory'):\n",
    "        cache_dir = Path(dspy.cache.disk_cache.directory)\n",
    "        print(f\"‚úÖ Cache directory from DSPy: {cache_dir}\")\n",
    "    elif hasattr(dspy.cache.disk_cache, '_directory'):\n",
    "        cache_dir = Path(dspy.cache.disk_cache._directory)\n",
    "        print(f\"‚úÖ Cache directory from DSPy: {cache_dir}\")\n",
    "\n",
    "if cache_dir and cache_dir.exists():\n",
    "    files = list(cache_dir.rglob('*'))\n",
    "    print(f\"\\nüìÅ Cache contains {len([f for f in files if f.is_file()])} files\")\n",
    "else:\n",
    "    print(\"‚ùå No cache directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cff5957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 largest cache files:\n",
      "1. 002/cache.db-wal\n",
      "   Size: 193.16 KB\n",
      "\n",
      "2. 000/cache.db-wal\n",
      "   Size: 193.16 KB\n",
      "\n",
      "3. 009/cache.db-wal\n",
      "   Size: 193.16 KB\n",
      "\n",
      "4. 007/cache.db-wal\n",
      "   Size: 181.09 KB\n",
      "\n",
      "5. 008/cache.db-wal\n",
      "   Size: 160.97 KB\n",
      "\n",
      "6. 012/cache.db-wal\n",
      "   Size: 160.97 KB\n",
      "\n",
      "7. 006/cache.db-wal\n",
      "   Size: 156.95 KB\n",
      "\n",
      "8. 013/cache.db-wal\n",
      "   Size: 156.95 KB\n",
      "\n",
      "9. 001/cache.db-wal\n",
      "   Size: 156.95 KB\n",
      "\n",
      "10. 010/cache.db-wal\n",
      "   Size: 136.83 KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Explore cache structure and list entries\n",
    "if cache_dir and cache_dir.exists():\n",
    "    cache_files = []\n",
    "    \n",
    "    for file in cache_dir.rglob('*'):\n",
    "        if file.is_file():\n",
    "            # Get file info\n",
    "            size_kb = file.stat().st_size / 1024\n",
    "            rel_path = file.relative_to(cache_dir)\n",
    "            cache_files.append({\n",
    "                'path': file,\n",
    "                'rel_path': str(rel_path),\n",
    "                'size_kb': size_kb,\n",
    "                'name': file.name\n",
    "            })\n",
    "    \n",
    "    # Sort by size to find the interesting ones\n",
    "    cache_files.sort(key=lambda x: x['size_kb'], reverse=True)\n",
    "    \n",
    "    print(f\"Top 10 largest cache files:\")\n",
    "    for i, cf in enumerate(cache_files[:10], 1):\n",
    "        print(f\"{i}. {cf['rel_path']}\")\n",
    "        print(f\"   Size: {cf['size_kb']:.2f} KB\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb0edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: cache.db-wal\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Try to load and examine cache entries\n",
    "import shelve\n",
    "import dbm\n",
    "\n",
    "def try_load_cache_file(file_path):\n",
    "    \"\"\"Try different methods to load a cache file\"\"\"\n",
    "    \n",
    "    # Method 1: Try as pickle\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return 'pickle', data\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Try as JSON\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return 'json', data\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 3: Try as shelve/dbm\n",
    "    try:\n",
    "        # Remove extension for shelve\n",
    "        base_path = str(file_path).rsplit('.', 1)[0] if '.' in str(file_path) else str(file_path)\n",
    "        db = shelve.open(base_path, 'r')\n",
    "        data = dict(db)\n",
    "        db.close()\n",
    "        return 'shelve', data\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 4: Try raw text\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "        return 'text', data\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Try to load the first cache file\n",
    "if cache_files:\n",
    "    test_file = cache_files[0]['path']\n",
    "    print(f\"Attempting to load: {test_file.name}\")\n",
    "    \n",
    "    format_type, data = try_load_cache_file(test_file)\n",
    "    \n",
    "    if format_type:\n",
    "        print(f\"‚úÖ Successfully loaded as: {format_type}\")\n",
    "        print(f\"Data type: {type(data)}\")\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            print(f\"Keys: {list(data.keys())[:5]}\")  # Show first 5 keys\n",
    "        elif isinstance(data, (list, tuple)):\n",
    "            print(f\"Length: {len(data)}\")\n",
    "        elif isinstance(data, str):\n",
    "            print(f\"String length: {len(data)} characters\")\n",
    "            print(f\"Preview: {data[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7e1588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Examining DSPy Memory Cache\n",
      "----------------------------------------\n",
      "Memory cache type: <class 'cachetools.LRUCache'>\n",
      "Number of entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Look at the in-memory cache (current session)\n",
    "print(\"üß† Examining DSPy Memory Cache\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Access memory cache\n",
    "memory_cache = dspy.cache.memory_cache\n",
    "\n",
    "print(f\"Memory cache type: {type(memory_cache)}\")\n",
    "print(f\"Number of entries: {len(memory_cache)}\")\n",
    "\n",
    "# Show first few entries\n",
    "if memory_cache:\n",
    "    for i, (key, value) in enumerate(list(memory_cache.items())[:3]):\n",
    "        print(f\"\\nüìå Entry {i+1}:\")\n",
    "        print(f\"Key type: {type(key)}\")\n",
    "        print(f\"Key preview: {str(key)[:100]}...\")\n",
    "        print(f\"Value type: {type(value)}\")\n",
    "        \n",
    "        # If value is a dict with prompt/response\n",
    "        if isinstance(value, dict):\n",
    "            for k, v in value.items():\n",
    "                if isinstance(v, str):\n",
    "                    print(f\"  {k}: {v[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"  {k}: {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7ce15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Extract actual prompts and responses from cache\n",
    "def examine_cache_entry(entry):\n",
    "    \"\"\"Extract and format cache entry for examination\"\"\"\n",
    "    \n",
    "    if isinstance(entry, dict):\n",
    "        # Look for common keys\n",
    "        prompt_keys = ['prompt', 'messages', 'input', 'query', 'instruction']\n",
    "        response_keys = ['response', 'output', 'completion', 'choices', 'text']\n",
    "        \n",
    "        found_prompt = None\n",
    "        found_response = None\n",
    "        \n",
    "        for key in prompt_keys:\n",
    "            if key in entry:\n",
    "                found_prompt = entry[key]\n",
    "                break\n",
    "        \n",
    "        for key in response_keys:\n",
    "            if key in entry:\n",
    "                found_response = entry[key]\n",
    "                break\n",
    "        \n",
    "        return found_prompt, found_response\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Examine entries in memory cache\n",
    "if memory_cache:\n",
    "    print(\"üîç Detailed Cache Analysis\\n\")\n",
    "    \n",
    "    for i, (cache_key, cache_value) in enumerate(list(memory_cache.items())[:5]):\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"CACHE ENTRY {i+1}\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        # Try to extract prompt and response\n",
    "        prompt, response = examine_cache_entry(cache_value)\n",
    "        \n",
    "        if prompt:\n",
    "            print(\"\\nüìù PROMPT/INPUT:\")\n",
    "            if isinstance(prompt, str):\n",
    "                print(prompt[:500])\n",
    "            elif isinstance(prompt, list):  # Messages format\n",
    "                for msg in prompt[:3]:\n",
    "                    print(f\"  {msg}\")\n",
    "        \n",
    "        if response:\n",
    "            print(\"\\nüí¨ RESPONSE/OUTPUT:\")\n",
    "            if isinstance(response, str):\n",
    "                print(response[:500])\n",
    "            elif isinstance(response, dict):\n",
    "                print(json.dumps(response, indent=2)[:500])\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5672fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Extract actual prompts and responses from cache\n",
    "def examine_cache_entry(entry):\n",
    "    \"\"\"Extract and format cache entry for examination\"\"\"\n",
    "    \n",
    "    if isinstance(entry, dict):\n",
    "        # Look for common keys\n",
    "        prompt_keys = ['prompt', 'messages', 'input', 'query', 'instruction']\n",
    "        response_keys = ['response', 'output', 'completion', 'choices', 'text']\n",
    "        \n",
    "        found_prompt = None\n",
    "        found_response = None\n",
    "        \n",
    "        for key in prompt_keys:\n",
    "            if key in entry:\n",
    "                found_prompt = entry[key]\n",
    "                break\n",
    "        \n",
    "        for key in response_keys:\n",
    "            if key in entry:\n",
    "                found_response = entry[key]\n",
    "                break\n",
    "        \n",
    "        return found_prompt, found_response\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Examine entries in memory cache\n",
    "if memory_cache:\n",
    "    print(\"üîç Detailed Cache Analysis\\n\")\n",
    "    \n",
    "    for i, (cache_key, cache_value) in enumerate(list(memory_cache.items())[:5]):\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"CACHE ENTRY {i+1}\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        # Try to extract prompt and response\n",
    "        prompt, response = examine_cache_entry(cache_value)\n",
    "        \n",
    "        if prompt:\n",
    "            print(\"\\nüìù PROMPT/INPUT:\")\n",
    "            if isinstance(prompt, str):\n",
    "                print(prompt[:500])\n",
    "            elif isinstance(prompt, list):  # Messages format\n",
    "                for msg in prompt[:3]:\n",
    "                    print(f\"  {msg}\")\n",
    "        \n",
    "        if response:\n",
    "            print(\"\\nüí¨ RESPONSE/OUTPUT:\")\n",
    "            if isinstance(response, str):\n",
    "                print(response[:500])\n",
    "            elif isinstance(response, dict):\n",
    "                print(json.dumps(response, indent=2)[:500])\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "634f84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching cache for 'SHALL'...\n",
      "\n",
      "Found 0 cache entries containing 'SHALL'\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Search cache for specific patterns or requirements\n",
    "search_term = \"SHALL\"  # Change this to search for different patterns\n",
    "\n",
    "found_entries = []\n",
    "\n",
    "print(f\"üîé Searching cache for '{search_term}'...\\n\")\n",
    "\n",
    "for key, value in memory_cache.items():\n",
    "    # Convert to string for searching\n",
    "    value_str = str(value)\n",
    "    \n",
    "    if search_term.lower() in value_str.lower():\n",
    "        found_entries.append((key, value))\n",
    "\n",
    "print(f\"Found {len(found_entries)} cache entries containing '{search_term}'\")\n",
    "\n",
    "# Show first match in detail\n",
    "if found_entries:\n",
    "    key, value = found_entries[0]\n",
    "    print(f\"\\nFirst matching entry:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pretty print if it's JSON-like\n",
    "    if isinstance(value, dict):\n",
    "        print(json.dumps(value, indent=2)[:1000])\n",
    "    else:\n",
    "        print(str(value)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0348073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Your Signature Definition:\n",
      "Class: ExtractReqs\n",
      "Docstring: You are a Federal proposal compliance analyst. Read all attached solicitation files (RFP Sections A‚ÄìM, SOW/PWS, CDRLs, amendments, attachments). Prioritize Section L (Instructions) and Section M (Evaluation).\n",
      "Deliver a BD-style ‚ÄúSubmission Compliance Checklist‚Äù workbook with the following tabs and schemas. Cite the exact source for every row as: DocName, Section/Para, Page. If anything is not stated, write ‚ÄúMISSING‚Äù (don‚Äôt guess).\n",
      "\n",
      "Tab 1 - Volumes & Tabs (authoritative spine)\n",
      "Columns: Volume, Tab, Title/Topic, What to Provide (verbatim/summary), Page/Slide Limit, Format (PDF/Word/PPT/Excel), Font/Margins/Spacing, Required File Naming, Submission Destination (Portal/Email), Due (Date, Time, Time Zone), Owner, Source Citation, Notes.\n",
      "- Capture exact volume/tab structure, mandatory contents per tab, and constraints.\n",
      "- Include file-type rules (allowed/prohibited), email size limits, labeling conventions (‚Äú1 of 3‚Äù), ZIP allowed?\n",
      "\n",
      "Tab 2 - Submission Logistics\n",
      "Columns: Due Date/Time/TZ, Questions Due, Addenda Cutoff, Submission Method (Portal/Email+Address/URL), Subject-Line Format, Copy To, Validity Period (e.g., 90 days), Place of Delivery, Special Packaging/Labeling, Source Citation.\n",
      "\n",
      "Tab 3 - Page/Slide & Formatting Rules\n",
      "Columns: Applies To (Vol/Tab/All), Page/Slide Limit, Font, Size (pt), Margins, Spacing, Figures/Tables Count Toward Limit?, Appendices Allowed?, Source Citation.\n",
      "\n",
      "Tab 4 - Orals (if applicable)\n",
      "Columns: Platform (Teams/Webex/In-person), Duration (Brief/Break/Q&A), Max Presenters/Attendees, Content Allowed/Prohibited (e.g., price), Recording Allowed?, Participant List Due (timing), Deck Submission Timing/Format, Source Citation.\n",
      "\n",
      "Tab 5 - Evaluation Matrix (Section M)\n",
      "Columns: Factor, Subfactor, Standard/What ‚ÄúAcceptable‚Äù Means, Relative Importance, Weight/Points (if given), Trade-off vs Price, Risk Considerations, Clarifying Notes, Source Citation.\n",
      "\n",
      "Tab 6 - Admin/Compliance (General)\n",
      "Columns: Topic, Requirement (verbatim/summary), Clause/Reference (FAR/DFARS/etc.), Applicability (All/Specific Volume), Source Citation, Notes.\n",
      "- Include: CDRL/DI- numbers, NAICS/PSC, contract type (FFP/T&M/etc.), PoP & place, security (CUI/NIST/ITAR), reviews/IBR/PMR, travel policy, gov‚Äôt portals, etc.\n",
      "\n",
      "Tab 7 - Pre-Submission Checklist\n",
      "Checkbox list derived from Tabs 1-6: Item, Owner, Status (Not Started/In Progress/Done), Due, Link to Evidence, Source Citation.\n",
      "\n",
      "Output format: Provide as a single Excel file; mirror the column headers exactly; no free-text prose instead of tables. No hallucinations‚Äîleave MISSING where the docs are silent.\n",
      "Quality gates:\n",
      "\n",
      "Every row must have a Source Citation.\n",
      "\n",
      "Page/slide limits and formats must be verbatim or clearly summarized from Section L.\n",
      "\n",
      "Where Section L conflicts with SOW, treat Section L as controlling and note the discrepancy in Notes.\n",
      "\n",
      "Surface red-flag risks (e.g., page cap conflicts, portal vs. email ambiguity) at the top of each tab.\n",
      "Fields: {'chunk_text': FieldInfo(annotation=str, required=True, json_schema_extra={'desc': 'The source text from a solicitation or instruction section.', '__dspy_field_type': 'input', 'prefix': 'Chunk Text:'}), 'requirements_json': FieldInfo(annotation=str, required=True, json_schema_extra={'desc': 'A JSON array of requirement objects with fields: id, label, category, modality, quote, section, page_start, page_end', 'prefix': 'JSON:', '__dspy_field_type': 'output'})}\n",
      "\n",
      "ü§ñ What DSPy Actually Sends:\n",
      "No extraction prompts found in cache yet. Run an extraction first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192783/525708050.py:9: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(f\"Fields: {ExtractReqs.__fields__}\")\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: See how DSPy constructs prompts from your signatures\n",
    "# This shows what DSPy actually sends vs what you defined\n",
    "\n",
    "from src.extraction.signatures import ExtractReqs\n",
    "\n",
    "print(\"üìã Your Signature Definition:\")\n",
    "print(f\"Class: {ExtractReqs.__name__}\")\n",
    "print(f\"Docstring: {ExtractReqs.__doc__}\")\n",
    "print(f\"Fields: {ExtractReqs.__fields__}\")\n",
    "\n",
    "print(\"\\nü§ñ What DSPy Actually Sends:\")\n",
    "\n",
    "# Look for entries that match your extraction calls\n",
    "extraction_entries = []\n",
    "\n",
    "for key, value in memory_cache.items():\n",
    "    value_str = str(value)\n",
    "    # Look for signature markers\n",
    "    if \"requirement\" in value_str.lower() and \"json\" in value_str.lower():\n",
    "        extraction_entries.append(value)\n",
    "\n",
    "if extraction_entries:\n",
    "    # Show the full prompt DSPy constructed\n",
    "    example = extraction_entries[0]\n",
    "    \n",
    "    if isinstance(example, dict) and 'messages' in example:\n",
    "        for msg in example['messages']:\n",
    "            print(f\"\\n[{msg.get('role', 'unknown').upper()}]:\")\n",
    "            print(msg.get('content', '')[:800])\n",
    "    else:\n",
    "        print(json.dumps(example, indent=2)[:1500])\n",
    "else:\n",
    "    print(\"No extraction prompts found in cache yet. Run an extraction first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8409fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported 0 cache entries to dspy_cache_analysis.csv\n",
      "\n",
      "Cache Summary:\n",
      "  Total entries: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'size_bytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCache Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total entries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cache_export)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msize_bytes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.sum()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m KB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Average entry size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[33m'\u001b[39m\u001b[33msize_bytes\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m bytes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fon_proposal_writer/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fon_proposal_writer/.venv/lib/python3.12/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'size_bytes'"
     ]
    }
   ],
   "source": [
    "# Cell 8: Export interesting cache entries to a file for deeper analysis\n",
    "import pandas as pd\n",
    "\n",
    "cache_export = []\n",
    "\n",
    "for i, (key, value) in enumerate(memory_cache.items()):\n",
    "    entry = {\n",
    "        'index': i,\n",
    "        'key_hash': str(key)[:50],\n",
    "        'type': type(value).__name__,\n",
    "        'size_bytes': len(str(value))\n",
    "    }\n",
    "    \n",
    "    # Try to extract meaningful content\n",
    "    if isinstance(value, dict):\n",
    "        entry['keys'] = list(value.keys())\n",
    "        if 'messages' in value:\n",
    "            entry['message_count'] = len(value['messages'])\n",
    "        if 'response' in value:\n",
    "            entry['has_response'] = True\n",
    "    \n",
    "    cache_export.append(entry)\n",
    "\n",
    "# Save to CSV for analysis\n",
    "df = pd.DataFrame(cache_export)\n",
    "output_file = 'dspy_cache_analysis.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Exported {len(cache_export)} cache entries to {output_file}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nCache Summary:\")\n",
    "print(f\"  Total entries: {len(cache_export)}\")\n",
    "print(f\"  Total size: {df['size_bytes'].sum() / 1024:.2f} KB\")\n",
    "print(f\"  Average entry size: {df['size_bytes'].mean():.2f} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fon_proposal_writer (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
