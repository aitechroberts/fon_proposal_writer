{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6097cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Basic imports successful\n",
      "Python version: 3.12.11 (main, Aug  8 2025, 17:06:48) [Clang 20.1.4 ]\n",
      "Current directory: /home/jroberts/fon_proposal_writer\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Core imports and path setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Basic imports successful\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d580cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_API_KEY: ✅ Set\n",
      "AZURE_API_BASE: ✅ Set\n",
      "AZURE_OPENAI_DEPLOYMENT: ✅ Set\n",
      "LANGFUSE_PUBLIC_KEY: ✅ Set\n",
      "LANGFUSE_SECRET_KEY: ✅ Set\n",
      "AZURE_STORAGE_CONNECTION_STRING: ✅ Set\n",
      "\n",
      "✅ All required environment variables are set\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and verify environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check critical environment variables\n",
    "env_vars = {\n",
    "    \"AZURE_API_KEY\": os.getenv(\"AZURE_API_KEY\"),\n",
    "    \"AZURE_API_BASE\": os.getenv(\"AZURE_API_BASE\"),\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"AZURE_STORAGE_CONNECTION_STRING\": os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\"),\n",
    "}\n",
    "\n",
    "# Display status (hide actual values)\n",
    "for key, value in env_vars.items():\n",
    "    status = \"✅ Set\" if value else \"❌ Missing\"\n",
    "    print(f\"{key}: {status}\")\n",
    "    \n",
    "missing = [k for k, v in env_vars.items() if not v]\n",
    "if missing:\n",
    "    print(f\"\\n⚠️ Missing environment variables: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\n✅ All required environment variables are set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da7567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Azure Deployment: gpt-4.1\n",
      "  Langfuse Host: https://us.cloud.langfuse.com\n",
      "  Blob Container: proposal-container\n",
      "  Debug Mode: False\n",
      "\n",
      "  Constructed endpoint: https://proposal-openai-model.openai.azure.com//openai/v1/\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test configuration module\n",
    "from src.config import settings\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Azure Deployment: {settings.azure_openai_deployment}\")\n",
    "print(f\"  Langfuse Host: {settings.langfuse_host}\")\n",
    "print(f\"  Blob Container: {settings.azure_blob_container}\")\n",
    "print(f\"  Debug Mode: {settings.debug}\")\n",
    "\n",
    "# Verify Azure OpenAI endpoint construction\n",
    "endpoint = f\"{settings.azure_api_base}/openai/v1/\"\n",
    "print(f\"\\n  Constructed endpoint: {endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e520e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "PROPOSAL_ZIP = \"Senate_Capitol_Facilities\"\n",
    "\n",
    "def download_zip_file():\n",
    "    \"\"\"Download zip file from Azure Blob Storage\"\"\"\n",
    "    \n",
    "    # Initialize the BlobServiceClient using connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(\n",
    "        os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    )\n",
    "    \n",
    "    # You need to know the container name - common names are:\n",
    "    # \"data\", \"files\", \"uploads\", \"rfp-poc\", etc.\n",
    "    container_name = \"proposal-container\"  # Replace with your actual container name\n",
    "    blob_name = f\"{PROPOSAL_ZIP}.zip\"\n",
    "    local_file_path = f\"./data/{PROPOSAL_ZIP}.zip\"  # Where to save locally\n",
    "    \n",
    "    try:\n",
    "        # Get blob client\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name, \n",
    "            blob=blob_name\n",
    "        )\n",
    "        \n",
    "        # Download the blob\n",
    "        print(f\"Downloading {blob_name}...\")\n",
    "        with open(local_file_path, \"wb\") as download_file:\n",
    "            download_data = blob_client.download_blob()\n",
    "            download_file.write(download_data.readall())\n",
    "        \n",
    "        print(f\"✅ Successfully downloaded to {local_file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading file: {e}\")\n",
    "        \n",
    "        # If container name is wrong, list available containers\n",
    "        if \"ContainerNotFound\" in str(e):\n",
    "            print(\"\\nAvailable containers:\")\n",
    "            for container in blob_service_client.list_containers():\n",
    "                print(f\"  - {container.name}\")\n",
    "\n",
    "# Run the download\n",
    "download_zip_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e887b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Unzip the file to current directory\n",
    "with zipfile.ZipFile(f'./data/{PROPOSAL_ZIP}.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data/inputs/')  # '.' means current directory\n",
    "\n",
    "print(\"✅ Unzipped all files to current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e412de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded PDF: data/inputs/DOE_Personnel/Attachment 1 - Administrative Background Investigation Support Services - PWS.1740156524666 (1).pdf\n",
      "  Number of pages: 10\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test PDF loader\n",
    "from src.io.loaders import pdf_to_pages\n",
    "\n",
    "# Create a test PDF or use an existing one\n",
    "test_pdf_path = \"data/inputs/DOE_Personnel/Attachment 1 - Administrative Background Investigation Support Services - PWS.1740156524666 (1).pdf\"\n",
    "\n",
    "if Path(test_pdf_path).exists():\n",
    "    pages = pdf_to_pages(test_pdf_path)\n",
    "    print(f\"✅ Successfully loaded PDF: {test_pdf_path}\")\n",
    "    print(f\"  Number of pages: {len(pages)}\")\n",
    "    # print(f\"  First page preview (first 500 chars):\")\n",
    "    # if pages:\n",
    "    #     print(f\"  {pages[0][1][:500]}...\")\n",
    "else:\n",
    "    print(f\"❌ Test PDF not found at: {test_pdf_path}\")\n",
    "    print(\"  Create a test PDF or update the path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "043919a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 10 page chunks (1 per page)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Convert pages to chunk format (no chunking, just formatting)\n",
    "# Skip the chunking step - use pages directly\n",
    "page_chunks = []\n",
    "for page_num, page_text in pages:\n",
    "    page_chunk = {\n",
    "        \"text\": page_text,\n",
    "        \"section\": f\"Page {page_num}\",  # Simple section identifier\n",
    "        \"start_page\": page_num,\n",
    "        \"end_page\": page_num\n",
    "    }\n",
    "    page_chunks.append(page_chunk)\n",
    "\n",
    "print(f\"✅ Created {len(page_chunks)} page chunks (1 per page)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17cd2a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 0 total regex matches across all pages\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run regex on full pages\n",
    "from src.preprocessing.regex_pass import fast_hits\n",
    "\n",
    "all_matches = []\n",
    "for page_chunk in page_chunks:\n",
    "    matches = fast_hits(page_chunk)\n",
    "    all_matches.extend(matches)\n",
    "    \n",
    "print(f\"✅ Found {len(all_matches)} total regex matches across all pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3487836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DSPy configured with model: azure/gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Initialize DSPy with Azure OpenAI\n",
    "import dspy\n",
    "from src.config import settings\n",
    "\n",
    "# Configure DSPy\n",
    "lm = dspy.LM(\n",
    "    model=settings.azure_openai_deployment,\n",
    "    api_key=settings.azure_api_key,\n",
    "    api_base=f\"{settings.azure_api_base}/openai/v1/\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=32700\n",
    ")\n",
    "\n",
    "# Set up with JSON adapter for structured output\n",
    "dspy.configure(lm=lm, adapter=dspy.JSONAdapter(), track_usage=False, cache=True)\n",
    "# dspy.configure_cache(\n",
    "#     enable_disk_cache=False,\n",
    "#     enable_memory_cache=False,\n",
    "# )\n",
    "\n",
    "print(f\"✅ DSPy configured with model: azure/{settings.azure_openai_deployment}\")\n",
    "\n",
    "# # Test with a simple prompt\n",
    "# test_lm = dspy.Predict(\"question -> answer\")\n",
    "# try:\n",
    "#     result = test_lm(question=\"What is 5+3?\")\n",
    "#     print(f\"✅ DSPy test successful: {result.answer}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ DSPy test failed: {e}\")\n",
    "#     print(\"API Base:\", settings.azure_api_base)\n",
    "#     print(\"Deployment:\", settings.azure_openai_deployment)\n",
    "#     print(\"API Key set:\", bool(settings.azure_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2a45b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 18:35:54,242 - src.observability.tracing - INFO - Tracing initialized with Langfuse host: https://us.cloud.langfuse.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Langfuse tracing initialized\n",
      "  Host: https://us.cloud.langfuse.com\n",
      "✅ Test trace created: Test successful\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Initialize Langfuse tracing\n",
    "from src.observability.tracing import initialize_tracing, get_langfuse_client\n",
    "\n",
    "try:\n",
    "    langfuse_client = initialize_tracing()\n",
    "    print(\"✅ Langfuse tracing initialized\")\n",
    "    print(f\"  Host: {settings.langfuse_host}\")\n",
    "    \n",
    "    # Create a test trace\n",
    "    from langfuse import observe\n",
    "    \n",
    "    @observe(name=\"test_function\")\n",
    "    def test_trace():\n",
    "        return \"Test successful\"\n",
    "    \n",
    "    result = test_trace()\n",
    "    langfuse_client.flush()\n",
    "    print(f\"✅ Test trace created: {result}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Langfuse initialization warning: {e}\")\n",
    "    print(\"  Continuing without tracing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0abc01a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 pages (pages 1-10)\n",
      "Processing page 1 (1/10)\n",
      "  → Found 6 requirements\n",
      "    • Contractor must provide personnel security support services to the Personnel Sec...\n",
      "    • Contractor must provide Team Lead (1 each) and Administrative Assistant (3 each)...\n",
      "Processing page 2 (2/10)\n",
      "  → Found 7 requirements\n",
      "    • Contractor shall provide qualified and skilled personnel, supervision, equipment...\n",
      "    • Contractor shall ensure all contractor employees are sufficiently qualified to p...\n",
      "Processing page 3 (3/10)\n",
      "  → Found 16 requirements\n",
      "    • Provide expertise and perform all necessary administrative services for NR Perso...\n",
      "    • Perform vault and PSF maintenance, clearance request processing, complete securi...\n",
      "Processing page 4 (4/10)\n",
      "  → Found 11 requirements\n",
      "    • Provide expertise and perform all necessary services to train personnel to perfo...\n",
      "    • All personnel shall meet training requirements for processing, and complete refr...\n",
      "Processing page 5 (5/10)\n",
      "  → Found 17 requirements\n",
      "    • Provide expertise and perform all necessary services for duplication or routine ...\n",
      "    • Administer and maintain all intra- and inter-office IMS for personnel security, ...\n",
      "Processing page 6 (6/10)\n",
      "  → Found 6 requirements\n",
      "    • Deliver specified number of copies of deliverables in media and electronic forma...\n",
      "    • Resubmit deliverables within two business days if changes or clarifications are ...\n",
      "Processing page 7 (7/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 22:14:28,477 - src.extraction.modules - ERROR - Failed to parse JSON from extractor: Expecting value: line 1 column 2 (char 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Found 0 requirements\n",
      "Processing page 8 (8/10)\n",
      "  → Found 10 requirements\n",
      "    • Provide timely notification (within 24 hours) of any deviation from records mana...\n",
      "    • All data entered into CATS, budget continuous evaluation or successor databases ...\n",
      "Processing page 9 (9/10)\n",
      "  → Found 3 requirements\n",
      "    • Provide 100% personnel coverage for personnel security services during core hour...\n",
      "    • Provide vault coverage (open and close) between specified hours...\n",
      "Processing page 10 (10/10)\n",
      "  → Found 22 requirements\n",
      "    • Compliance with Title 10 CFR Part 851 – Worker Safety and Health Program...\n",
      "    • Compliance with Title 48 CFR Part 22 – Application of Labor Laws to Government A...\n",
      "\n",
      "✅ Total extracted: 98 requirements from 10 pages\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Process pages through language model (process specific pages or all)\n",
    "from src.extraction.modules import Extractor\n",
    "import json\n",
    "\n",
    "extractor = Extractor()\n",
    "\n",
    "# Process specific pages (e.g., last 10 pages which often have requirements)\n",
    "pages_to_process = page_chunks  # Last 10 pages, or use page_chunks[:10] for first 10\n",
    "print(f\"Processing {len(pages_to_process)} pages (pages {pages_to_process[0]['start_page']}-{pages_to_process[-1]['end_page']})\")\n",
    "\n",
    "all_extracted = []\n",
    "for i, page_chunk in enumerate(pages_to_process):\n",
    "    print(f\"Processing page {page_chunk['start_page']} ({i+1}/{len(pages_to_process)})\")\n",
    "    try:\n",
    "        requirements = extractor(page_chunk)\n",
    "        all_extracted.extend(requirements)\n",
    "        print(f\"  → Found {len(requirements)} requirements\")\n",
    "        for req in requirements[:2]:  # Show first 2 from each page\n",
    "            print(f\"    • {req.get('label', 'No label')[:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Extraction failed: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Total extracted: {len(all_extracted)} requirements from {len(pages_to_process)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f3d45fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache methods and attributes:\n",
      "  cache_key\n",
      "  disk_cache\n",
      "  enable_disk_cache\n",
      "  enable_memory_cache\n",
      "  get\n",
      "  load_memory_cache\n",
      "  memory_cache\n",
      "  put\n",
      "  reset_memory_cache\n",
      "  save_memory_cache\n",
      "\n",
      "All cache attributes (including private):\n",
      "  _lock: <class '_thread.RLock'>\n",
      "  disk_cache: <class 'diskcache.fanout.FanoutCache'>\n",
      "  enable_disk_cache: <class 'bool'>\n",
      "  enable_memory_cache: <class 'bool'>\n",
      "  memory_cache: <class 'cachetools.LRUCache'>\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "# See what methods the cache has\n",
    "print(\"Cache methods and attributes:\")\n",
    "for attr in dir(dspy.cache):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"  {attr}\")\n",
    "\n",
    "# Check for common cache patterns\n",
    "if hasattr(dspy.cache, 'cache'):\n",
    "    print(\"\\nFound cache.cache - type:\", type(dspy.cache.cache))\n",
    "    if hasattr(dspy.cache.cache, 'clear'):\n",
    "        dspy.cache.cache.clear()\n",
    "        print(\"✓ Cleared dspy.cache.cache\")\n",
    "\n",
    "if hasattr(dspy.cache, '_cache'):\n",
    "    print(\"\\nFound cache._cache - type:\", type(dspy.cache._cache))\n",
    "    if hasattr(dspy.cache._cache, 'clear'):\n",
    "        dspy.cache._cache.clear()\n",
    "        print(\"✓ Cleared dspy.cache._cache\")\n",
    "\n",
    "if hasattr(dspy.cache, 'data'):\n",
    "    print(\"\\nFound cache.data - type:\", type(dspy.cache.data))\n",
    "    if hasattr(dspy.cache.data, 'clear'):\n",
    "        dspy.cache.data.clear()\n",
    "        print(\"✓ Cleared dspy.cache.data\")\n",
    "\n",
    "# Try to see the internal structure\n",
    "print(\"\\nAll cache attributes (including private):\")\n",
    "for attr in dir(dspy.cache):\n",
    "    try:\n",
    "        value = getattr(dspy.cache, attr)\n",
    "        if not callable(value) and not attr.startswith('__'):\n",
    "            print(f\"  {attr}: {type(value)}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ca6b6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cache size: 1\n",
      "Disk cache size: 1\n",
      "✓ Memory cache reset\n",
      "✓ Disk cache cleared\n",
      "Memory cache size: 0\n",
      "Disk cache size: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory cache size: {len(dspy.cache.memory_cache)}\")\n",
    "print(f\"Disk cache size: {len(dspy.cache.disk_cache)}\")\n",
    "\n",
    "\n",
    "# 1. Clear the memory cache\n",
    "dspy.cache.reset_memory_cache()\n",
    "print(\"✓ Memory cache reset\")\n",
    "\n",
    "# 2. Clear the disk cache\n",
    "dspy.cache.disk_cache.clear()\n",
    "print(\"✓ Disk cache cleared\")\n",
    "\n",
    "print(f\"Memory cache size: {len(dspy.cache.memory_cache)}\")\n",
    "print(f\"Disk cache size: {len(dspy.cache.disk_cache)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5674854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted requirements saved to ./test-results/extractor_rfp_letter_3.json\n"
     ]
    }
   ],
   "source": [
    "test_type = \"extractor_rfp_letter\"\n",
    "test_number = 3\n",
    "with open(f\"./test-results/{test_type}_test_{str(test_number)}.json\", \"x\") as f:\n",
    "    json.dump(all_extracted, f, indent=2)\n",
    "\n",
    "print(f\"Extracted requirements saved to ./test-results/{test_type}_{str(test_number)}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd41353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Simple conversion:\n",
      "✅ Converted ./test-results/extractor_test_2.json to requirements_simple.xlsx\n",
      "📊 29 rows exported\n",
      "\n",
      "✨ Formatted conversion:\n",
      "✅ Converted ./test-results/extractor_test_2.json to requirements_formatted.xlsx (with formatting)\n",
      "📊 29 rows exported\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "\n",
    "def json_to_excel_simple(json_file_path, excel_file_path):\n",
    "    \"\"\"\n",
    "    Convert JSON file to Excel - simple and clean.\n",
    "    \"\"\"\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Write to Excel\n",
    "    df.to_excel(excel_file_path, index=False, sheet_name='Requirements')\n",
    "    \n",
    "    print(f\"✅ Converted {json_file_path} to {excel_file_path}\")\n",
    "    print(f\"📊 {len(data)} rows exported\")\n",
    "    return df\n",
    "\n",
    "def json_to_excel_formatted(json_file_path, excel_file_path):\n",
    "    \"\"\"\n",
    "    Convert JSON file to Excel with nice formatting.\n",
    "    \"\"\"\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Write to Excel with formatting\n",
    "    with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name='Requirements')\n",
    "        \n",
    "        # Get the workbook and worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Requirements']\n",
    "        \n",
    "        # Format headers\n",
    "        header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "        header_font = Font(bold=True, color='FFFFFF')\n",
    "        \n",
    "        for cell in worksheet[1]:\n",
    "            cell.fill = header_fill\n",
    "            cell.font = header_font\n",
    "            cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "        \n",
    "        # Auto-adjust column widths\n",
    "        for column in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column_letter = column[0].column_letter\n",
    "            \n",
    "            for cell in column:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(str(cell.value))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Set reasonable max width\n",
    "            adjusted_width = min(max_length + 2, 50)\n",
    "            worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        # Wrap text for long content columns\n",
    "        for row in worksheet.iter_rows(min_row=2):\n",
    "            for cell in row:\n",
    "                if cell.column_letter in ['B', 'E']:  # label and quote columns\n",
    "                    cell.alignment = Alignment(wrap_text=True, vertical='top')\n",
    "    \n",
    "    print(f\"✅ Converted {json_file_path} to {excel_file_path} (with formatting)\")\n",
    "    print(f\"📊 {len(data)} rows exported\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Simple conversion\n",
    "print(\"🚀 Simple conversion:\")\n",
    "df = json_to_excel_simple('./test-results/extractor_test_2.json', 'requirements_simple.xlsx')\n",
    "\n",
    "# Formatted conversion\n",
    "print(\"\\n✨ Formatted conversion:\")\n",
    "df = json_to_excel_formatted('./test-results/extractor_test_2.json', 'requirements_formatted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e33c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Classify all extracted requirements\n",
    "from src.extraction.modules import Classifier\n",
    "\n",
    "classifier = Classifier()\n",
    "classified_reqs = []\n",
    "\n",
    "print(f\"Classifying {len(all_extracted)} requirements...\\n\")\n",
    "\n",
    "for i, req in enumerate(all_extracted):\n",
    "    try:\n",
    "        classified = classifier(req)\n",
    "        classified_reqs.append(classified)\n",
    "        if (i % 5) == 0:  # Show first 5 classifications\n",
    "            print(f\"Requirement {i+1}:{len(all_extracted)}\")\n",
    "            # print(f\"  Label: {classified.get('label', 'Unknown')[:80]}...\")\n",
    "            # print(f\"  Category: {classified.get('category', 'Unknown')}\")\n",
    "            # print(f\"  Modality: {classified.get('modality', 'Unknown')}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Classification failed for requirement {i+1}: {e}\")\n",
    "        classified_reqs.append(req)  # Keep original if classification fails\n",
    "\n",
    "print(f\"✅ Classified {len(classified_reqs)} requirements\")\n",
    "\n",
    "# Show distribution\n",
    "from collections import Counter\n",
    "categories = Counter(r.get('category', 'Unknown') for r in classified_reqs)\n",
    "modalities = Counter(r.get('modality', 'Unknown') for r in classified_reqs)\n",
    "\n",
    "print(\"\\nCategory distribution:\")\n",
    "for cat, count in categories.most_common():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "print(\"\\nModality distribution:\")\n",
    "for mod, count in modalities.most_common():\n",
    "    print(f\"  {mod}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ed58cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted requirements saved to ./test-results/classifier_reqs_rfp_letter.json_3.json\n"
     ]
    }
   ],
   "source": [
    "test_type = \"classifier_reqs_rfp_letter.json\"\n",
    "test_number = 3\n",
    "with open(f\"./test-results/{test_type}_test_{str(test_number)}.json\", \"x\") as f:\n",
    "    json.dump(classified_reqs, f, indent=2)\n",
    "\n",
    "print(f\"Extracted requirements saved to ./test-results/{test_type}_{str(test_number)}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b62a8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "print(len(classified_reqs))\n",
    "print(len(all_extracted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Ground requirements with evidence from source chunks\n",
    "from src.extraction.modules import Grounder\n",
    "\n",
    "grounder = Grounder()\n",
    "grounded_reqs = []\n",
    "\n",
    "# Map requirements back to their source chunks\n",
    "print(f\"Grounding {len(classified_reqs)} requirements with evidence...\\n\")\n",
    "\n",
    "for i, req in enumerate(classified_reqs):\n",
    "    # Find the source chunk (assuming requirements maintain order)\n",
    "    chunk_index = min(i // 2, len(pages_to_process) - 1)  # Rough mapping\n",
    "    source_chunk = pages_to_process[chunk_index]\n",
    "    \n",
    "    try:\n",
    "        grounded = grounder(source_chunk, req)\n",
    "        grounded_reqs.append(grounded)\n",
    "        if (i % 3) == 0:  # Show first 3 grounded requirements\n",
    "            print(f\"Requirement {i+1}:\")\n",
    "            # print(f\"  Label: {grounded.get('label', 'Unknown')[:60]}...\")\n",
    "            # print(f\"  Evidence: {grounded.get('quote', 'No quote')[:100]}...\")\n",
    "            # print(f\"  Pages: {grounded.get('page_start', '?')}-{grounded.get('page_end', '?')}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Grounding failed for requirement {i+1}: {e}\")\n",
    "        grounded_reqs.append(req)\n",
    "\n",
    "# Add remaining requirements without grounding\n",
    "grounded_reqs.extend(classified_reqs[20:])\n",
    "\n",
    "print(f\"✅ Grounded {len(grounded_reqs)} requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f75cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted requirements saved to ./test-results/grounder_3.json\n",
      "Extracted requirements saved to ./test-results/grounder_reqs_3.json\n"
     ]
    }
   ],
   "source": [
    "test_type = \"grounder\"\n",
    "test_number = 3\n",
    "with open(f\"./test-results/{test_type}_test_{str(test_number)}.json\", \"x\") as f:\n",
    "    json.dump(all_extracted, f, indent=2)\n",
    "\n",
    "print(f\"Extracted requirements saved to ./test-results/{test_type}_{str(test_number)}.json\")\n",
    "\n",
    "test_type = \"grounder_reqs\"\n",
    "test_number = 3\n",
    "with open(f\"./test-results/{test_type}_test_{str(test_number)}.json\", \"x\") as f:\n",
    "    json.dump(grounded_reqs, f, indent=2)\n",
    "\n",
    "print(f\"Extracted requirements saved to ./test-results/{test_type}_{str(test_number)}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba38385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dedupe Summary ===\n",
      "Input files           : 2\n",
      "Total input records   : 617\n",
      "Final canonical count : 577\n",
      "Duplicates removed    : 40\n",
      "Wrote JSON -> canonical_requirements.json\n",
      "Wrote CSV  -> canonical_requirements.csv\n",
      "\n",
      "Top merged (id, sources_count, label snippet):\n",
      "- 8  x4  :: Contractor must propose qualified replacement for key personnel within 30 days o\n",
      "- 4.5.5  x3  :: Contractor SHALL determine employee's options under FEHB.\n",
      "- 6  x2  :: Present information in oral presentation or PowerPoint to facilitate evaluation \n",
      "- 4  x2  :: Clauses incorporated by reference\n",
      "- D.1-1  x2  :: Initial and annual privacy training for employees with PII or system of records \n",
      "- 1  x2  :: Contractor must maintain and provide privacy training completion documentation u\n",
      "- 2  x2  :: Employee access to PII or system of records requires completed privacy training.\n",
      "- 3  x2  :: Include privacy training clause in all subcontracts where subcontractor employee\n",
      "- 6  x2  :: Contractor must submit list of employee candidates with favorably adjudicated T4\n",
      "- 1  x2  :: Require contractor employee candidates needing T4/T5 background investigation to\n",
      "- 2  x2  :: Contractor ensures all Contract Employee candidates complete e-QIP and electroni\n",
      "- 3  x2  :: Ensure contract employee candidates respond promptly to CBP Security Office and \n",
      "- 4  x2  :: Contractor must submit forms, including clearance documents if applicable, to th\n",
      "- 5  x2  :: Contractor shall propose a qualified replacement employee candidate to the CO an\n",
      "- 6  x2  :: Provide names of employees who complete CBP T4 or T5 process to CO and COR.\n"
     ]
    }
   ],
   "source": [
    "# Test your existing src/extraction/merge_dedupe.py with confidence normalization\n",
    "\n",
    "import json, csv, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from src.extraction.merge_dedupe import merge_dedupe\n",
    "\n",
    "# <<< EDIT THESE >>>\n",
    "INPUT_FILES = [\n",
    "    \"test-results/classifier_reqs_rfp_letter.json_test_3.json\",\n",
    "    \"test-results/classifier_reqs_test_3.json\",\n",
    "]\n",
    "OUT_JSON = \"canonical_requirements.json\"\n",
    "OUT_CSV  = \"canonical_requirements.csv\"\n",
    "\n",
    "def _read_any_json(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        for key in (\"requirements\", \"items\", \"data\", \"records\", \"rows\"):\n",
    "            if key in data and isinstance(data[key], list):\n",
    "                return data[key]\n",
    "    return [data]\n",
    "\n",
    "def _to_confidence(v) -> float | None:\n",
    "    if v is None: \n",
    "        return None\n",
    "    if isinstance(v, (int, float)): \n",
    "        return float(v)\n",
    "    if isinstance(v, bool):\n",
    "        return 1.0 if v else 0.0\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip().lower()\n",
    "        # percent \"85%\"\n",
    "        if s.endswith(\"%\"):\n",
    "            try:\n",
    "                return float(s[:-1]) / 100.0\n",
    "            except Exception:\n",
    "                return None\n",
    "        # numeric-in-string \"0.92\" / \"1\"\n",
    "        try:\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # labels\n",
    "        if s in {\"very high\",\"vh\"}: return 0.99\n",
    "        if s in {\"high\",\"h\",\"strong\"}: return 0.95\n",
    "        if s in {\"medium\",\"med\",\"moderate\"}: return 0.65\n",
    "        if s in {\"low\",\"l\",\"weak\"}: return 0.35\n",
    "        if s in {\"very low\",\"vl\"}: return 0.15\n",
    "    return None\n",
    "\n",
    "def _coerce_for_merge(records: List[Dict[str, Any]], default_doc: str) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for rec in records:\n",
    "        if not isinstance(rec, dict):\n",
    "            continue\n",
    "        r = dict(rec)  # shallow copy\n",
    "        # Source/doc\n",
    "        r[\"doc\"] = r.get(\"doc\") or r.get(\"source\") or default_doc\n",
    "        r[\"source\"] = r.get(\"source\") or r[\"doc\"]\n",
    "        # Fields merge_dedupe expects\n",
    "        r[\"label\"] = r.get(\"label\") or r.get(\"title\") or r.get(\"requirement_label\") or \"\"\n",
    "        r[\"quote\"] = r.get(\"quote\") or r.get(\"text\") or r.get(\"requirement_text\") or r.get(\"chunk_text\") or \"\"\n",
    "        r[\"category\"] = r.get(\"category\") or r.get(\"topic\") or \"\"\n",
    "        r[\"section\"]  = r.get(\"section\") or r.get(\"heading\") or r.get(\"section_path\")\n",
    "        if \"page_start\" not in r or r.get(\"page_start\") is None:\n",
    "            r[\"page_start\"] = r.get(\"page\") or r.get(\"page_no\")\n",
    "        # Normalize confidence -> float\n",
    "        r[\"confidence\"] = _to_confidence(r.get(\"confidence\"))\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "# Load + coerce (including confidence normalization)\n",
    "rows: List[Dict[str, Any]] = []\n",
    "for p in INPUT_FILES:\n",
    "    p = Path(p)\n",
    "    rows.extend(_coerce_for_merge(_read_any_json(str(p)), default_doc=p.stem))\n",
    "\n",
    "before = len(rows)\n",
    "canon  = merge_dedupe(rows)\n",
    "after  = len(canon)\n",
    "\n",
    "# Save outputs\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(canon, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Lightweight CSV (no pandas required)\n",
    "fieldnames = sorted({k for r in canon for k in r.keys()})\n",
    "if \"sources\" not in fieldnames:\n",
    "    fieldnames.append(\"sources\")\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in canon:\n",
    "        row = dict(r)\n",
    "        row[\"sources\"] = json.dumps(r.get(\"sources\", []), ensure_ascii=False)\n",
    "        w.writerow(row)\n",
    "\n",
    "print(\"=== Dedupe Summary ===\")\n",
    "print(f\"Input files           : {len(INPUT_FILES)}\")\n",
    "print(f\"Total input records   : {before}\")\n",
    "print(f\"Final canonical count : {after}\")\n",
    "print(f\"Duplicates removed    : {before - after}\")\n",
    "print(f\"Wrote JSON -> {OUT_JSON}\")\n",
    "print(f\"Wrote CSV  -> {OUT_CSV}\")\n",
    "\n",
    "# Optional: quick look at which items merged multiple sources\n",
    "top = sorted(canon, key=lambda x: len(x.get(\"sources\", [])), reverse=True)[:15]\n",
    "print(\"\\nTop merged (id, sources_count, label snippet):\")\n",
    "for r in top:\n",
    "    print(f\"- {r.get('id')}  x{len(r.get('sources', []))}  :: {(r.get('label') or '')[:80]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5696eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote /home/jroberts/fon_proposal_writer/DHS_CBP_Requirements.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "PREFERRED_ORDER = [\n",
    "    \"id\", \"label\", \"category\", \"modality\",\n",
    "    \"quote\", \"section\", \"page_start\", \"page_end\", \"confidence\"\n",
    "]\n",
    "\n",
    "def load_json_array(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load a JSON file expected to contain a list[dict].\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(f\"{path} does not contain a JSON array.\")\n",
    "        return data\n",
    "\n",
    "def union_columns(*rows_lists: List[List[Dict[str, Any]]]) -> List[str]:\n",
    "    \"\"\"Union of keys across all objects, with a friendly preferred order.\"\"\"\n",
    "    keys = set()\n",
    "    for rows in rows_lists:\n",
    "        for r in rows:\n",
    "            keys.update(r.keys())\n",
    "    # Put known keys first (if present), then any extras in alphabetical order\n",
    "    extras = [k for k in sorted(keys) if k not in PREFERRED_ORDER]\n",
    "    ordered = [k for k in PREFERRED_ORDER if k in keys] + extras\n",
    "    return ordered\n",
    "\n",
    "def drop_duplicates(rows: List[Dict[str, Any]], on: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Optionally dedupe by one or more keys (e.g., ['id'] or ['quote']).\"\"\"\n",
    "    if not on:\n",
    "        return rows\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        key = tuple(r.get(k) for k in on)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "def coerce_int(v: Any) -> Any:\n",
    "    try:\n",
    "        if v is None or v == \"\":\n",
    "            return None\n",
    "        # Only coerce if it's numeric-ish\n",
    "        return int(v)\n",
    "    except Exception:\n",
    "        return v\n",
    "\n",
    "def write_excel(rows: List[Dict[str, Any]], columns: List[str], out_path: str) -> Path:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"requirements\"\n",
    "\n",
    "    # Header\n",
    "    ws.append(columns)\n",
    "\n",
    "    # Rows\n",
    "    for r in rows:\n",
    "        row = []\n",
    "        for c in columns:\n",
    "            val = r.get(c)\n",
    "            if c in (\"page_start\", \"page_end\"):\n",
    "                val = coerce_int(val)\n",
    "            row.append(val)\n",
    "        ws.append(row)\n",
    "\n",
    "    # Tidy column widths (simple heuristic)\n",
    "    for idx, col_name in enumerate(columns, start=1):\n",
    "        max_len = len(col_name)\n",
    "        for cell in ws[get_column_letter(idx)]:\n",
    "            if cell.value is not None:\n",
    "                max_len = max(max_len, len(str(cell.value)))\n",
    "        ws.column_dimensions[get_column_letter(idx)].width = min(max_len + 2, 60)\n",
    "\n",
    "    out = Path(out_path)\n",
    "    wb.save(out)\n",
    "    return out\n",
    "\n",
    "def combine_json_to_excel(\n",
    "    json_path_1: str,\n",
    "    json_path_2: str,\n",
    "    out_path: str = \"combined.xlsx\",\n",
    "    dedupe_on: Optional[List[str]] = None,   # e.g., [\"id\"] or [\"quote\"]\n",
    ") -> Path:\n",
    "    rows1 = load_json_array(json_path_1)\n",
    "    rows2 = load_json_array(json_path_2)\n",
    "\n",
    "    all_rows = rows1 + rows2\n",
    "    if dedupe_on:\n",
    "        all_rows = drop_duplicates(all_rows, on=dedupe_on)\n",
    "\n",
    "    cols = union_columns(all_rows)\n",
    "    return write_excel(all_rows, cols, out_path)\n",
    "\n",
    "output = combine_json_to_excel(\"test-results/DHS_CBP_RFP_Letter_classifier_reqs.json\", \"test-results/DHS_CBP_SOW_classifier_reqs.json\", out_path=\"DHS_CBP_Requirements.xlsx\")\n",
    "print(f\"✅ Wrote {output.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fon_proposal_writer (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
